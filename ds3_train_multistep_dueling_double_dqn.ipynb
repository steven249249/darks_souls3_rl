{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a861460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gymnasium \n",
    "import soulsgym\n",
    "import logging\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from transformation import GameStateTransformer\n",
    "import gym, random, pickle, os.path, math, glob\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import os\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be2de92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nstep_Memory_Buffer(object):\n",
    "    # memory buffer to store episodic memory\n",
    "    def __init__(self, memory_size=1000, n_multi_step = 1, gamma = 0.99):\n",
    "        self.buffer = []\n",
    "        self.memory_size = memory_size\n",
    "        self.n_multi_step = n_multi_step\n",
    "        self.gamma = gamma\n",
    "        self.next_idx = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        data = (state, action, reward, next_state, done)\n",
    "        if len(self.buffer) <= self.memory_size: # buffer not full\n",
    "            self.buffer.append(data)\n",
    "        else: # buffer is full\n",
    "            self.buffer[self.next_idx] = data\n",
    "        self.next_idx = (self.next_idx + 1) % self.memory_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # sample episodic memory\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        for i in range(batch_size):\n",
    "            finish = random.randint(self.n_multi_step, self.size() - 1)\n",
    "            begin = finish-self.n_multi_step\n",
    "            sum_reward = 0 # n_step rewards\n",
    "            data = self.buffer[begin:finish]\n",
    "            state = data[0][0]\n",
    "            action = data[0][1]\n",
    "            for j in range(self.n_multi_step):\n",
    "                # compute the n-th reward\n",
    "                sum_reward += (self.gamma**j) * data[j][2]\n",
    "                if data[j][4]:\n",
    "                    # manage end of episode\n",
    "                    states_look_ahead = data[j][3]\n",
    "                    done_look_ahead = True\n",
    "                    break\n",
    "                else:\n",
    "                    states_look_ahead = data[j][3]\n",
    "                    done_look_ahead = False\n",
    "            \n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(sum_reward)\n",
    "            next_states.append(states_look_ahead)\n",
    "            dones.append(done_look_ahead)\n",
    "\n",
    "        return np.concatenate(states), actions, rewards, np.concatenate(next_states), dones\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "286881e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self,input_dims,output_dims,layer_dims):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(input_dims, layer_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dims, layer_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dims, layer_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dims, output_dims)\n",
    "        )\n",
    "        \n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(input_dims, layer_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dims, layer_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dims, layer_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dims, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Compute the forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x: Network input.\n",
    "\n",
    "        Returns:\n",
    "            The network output.\n",
    "        \"\"\"\n",
    "       \n",
    "        advantage = self.advantage(x)\n",
    "        value     = self.value(x)\n",
    "        return value + advantage  - advantage.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "775fa9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nstep_DQNAgent: \n",
    "    def __init__(self, in_channels = 71, action_space = [], USE_CUDA =True, memory_size = 100000, n_multi_step = 4, gamma = 0.99, epsilon  = 1, lr = 1e-3):\n",
    "        self.epsilon = epsilon\n",
    "        self.action_space = action_space\n",
    "        self.n_multi_step = n_multi_step\n",
    "        self.gamma = gamma \n",
    "        self.memory_buffer = Nstep_Memory_Buffer(memory_size, n_multi_step = n_multi_step, gamma = gamma)\n",
    "        self.DQN = DQN(in_channels, 20,128)\n",
    "        self.DQN_target = DQN(in_channels, 20, 128)\n",
    "        self.DQN_target.load_state_dict(self.DQN.state_dict())\n",
    "\n",
    "\n",
    "        self.USE_CUDA = USE_CUDA\n",
    "        if USE_CUDA:\n",
    "            self.DQN = self.DQN.cuda()\n",
    "            self.DQN_target = self.DQN_target.cuda()\n",
    "        self.optimizer = optim.RMSprop(self.DQN.parameters(),lr=lr, eps=0.001, alpha=0.95)\n",
    "    def value(self, state):\n",
    "        state = torch.from_numpy(state).cuda()\n",
    "        q_values = self.DQN(state)\n",
    "        return q_values\n",
    "    \n",
    "    def act(self, state, epsilon = None,action_mask = None):\n",
    "        \"\"\"\n",
    "        sample actions with epsilon-greedy policy\n",
    "        recap: with p = epsilon pick random action, else pick action with highest Q(s,a)\n",
    "        \"\"\"\n",
    "        if epsilon is None: \n",
    "            epsilon = self.epsilon\n",
    "        c = torch.as_tensor(action_mask, dtype=torch.bool).cuda()\n",
    "        actions_value = self.value(state).cuda()\n",
    "        actions_value = torch.where(c, actions_value, torch.tensor([-torch.inf], dtype=torch.float32).cuda())\n",
    "        actions_value  = actions_value.cpu().detach().numpy()\n",
    "        if random.random()<epsilon:\n",
    "            action = random.randrange(self.action_space.n)\n",
    "        else:\n",
    "\n",
    "            action = actions_value.argmax(0)\n",
    "        if type(action)!=int:\n",
    "            action = np.int64(action)\n",
    "        return action\n",
    "    \n",
    "    def compute_td_loss(self, states, actions, rewards, next_states, is_done, gamma=0.99):\n",
    "        \"\"\" Compute td loss using torch operations only. Use the formula above. \"\"\"\n",
    "        actions = torch.tensor(actions).long()    # shape: [batch_size]\n",
    "        rewards = torch.tensor(rewards, dtype =torch.float)  # shape: [batch_size]\n",
    "        is_done = torch.tensor(is_done).bool()  # shape: [batch_size]\n",
    "        \n",
    "        if self.USE_CUDA:\n",
    "            actions = actions.cuda()\n",
    "            rewards = rewards.cuda()\n",
    "            is_done = is_done.cuda()\n",
    "            states = torch.reshape(states, (-1, 71)).cuda()\n",
    "            next_states = torch.reshape(next_states, (-1, 71)).cuda()\n",
    "\n",
    "        # get q-values for all actions in current states\n",
    "        predicted_qvalues = self.DQN(states)\n",
    "\n",
    "        # select q-values for chosen actions\n",
    "        predicted_qvalues_for_actions = predicted_qvalues[\n",
    "          range(states.shape[0]), actions\n",
    "        ]\n",
    "\n",
    "        # compute q-values for all actions in next states\n",
    "\n",
    "        \n",
    "        \n",
    "        predicted_next_qvalues_current = self.DQN(next_states)\n",
    "        predicted_next_qvalues_target = self.DQN_target(next_states)\n",
    "        # compute V*(next_states) using predicted next q-values\n",
    "        next_state_values =  predicted_next_qvalues_target.gather(1, torch.max(predicted_next_qvalues_current, 1)[1].unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "        target_qvalues_for_actions = rewards + (self.gamma**self.n_multi_step) *next_state_values\n",
    "\n",
    "        # at the last state we shall use simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "        target_qvalues_for_actions = torch.where(\n",
    "            is_done, rewards, target_qvalues_for_actions)\n",
    "\n",
    "        # mean squared error loss to minimize\n",
    "        #loss = torch.mean((predicted_qvalues_for_actions -\n",
    "        #                   target_qvalues_for_actions.detach()) ** 2)\n",
    "        loss = F.smooth_l1_loss(predicted_qvalues_for_actions, target_qvalues_for_actions.detach())\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def sample_from_buffer(self, batch_size):\n",
    "        # sample episodic memory\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        for i in range(batch_size):\n",
    "            finish = random.randint(self.n_multi_step, self.memory_buffer.size() - 1)\n",
    "            begin = finish-self.n_multi_step\n",
    "            sum_reward = 0 # n_step rewards\n",
    "            data = self.memory_buffer.buffer[begin:finish]\n",
    "            state = data[0][0] # s0\n",
    "            action = data[0][1] # a0\n",
    "            for j in range(self.n_multi_step):\n",
    "                # compute the n-th reward\n",
    "                sum_reward += (self.gamma**j) * data[j][2] # sum reward\n",
    "                if data[j][4]:\n",
    "                    # manage end of episode\n",
    "                    states_look_ahead = data[j][3] # st\n",
    "                    done_look_ahead = True\n",
    "                    break\n",
    "                else:\n",
    "                    states_look_ahead = data[j][3] # st\n",
    "                    done_look_ahead = False\n",
    "            \n",
    "            states.append(torch.from_numpy(state))\n",
    "            actions.append(action)\n",
    "            rewards.append(sum_reward)\n",
    "            next_states.append(torch.from_numpy(states_look_ahead))\n",
    "            dones.append(done_look_ahead)\n",
    "\n",
    "        return torch.cat(states), actions, rewards, torch.cat(next_states), dones\n",
    "\n",
    "    def learn_from_experience(self, batch_size):\n",
    "        if self.memory_buffer.size() > batch_size:\n",
    "            states, actions, rewards, next_states, dones = self.sample_from_buffer(batch_size)\n",
    "            td_loss = self.compute_td_loss(states, actions, rewards, next_states, dones)\n",
    "            self.optimizer.zero_grad()\n",
    "            td_loss.backward()\n",
    "            for param in self.DQN.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "            self.optimizer.step()\n",
    "            return(td_loss.item())\n",
    "        else:\n",
    "            return(0)\n",
    "    def save_model(self,n):\n",
    "        eval_net_path = './model{}.pth'.format(str(n))\n",
    "        target_net_path = './model_target{}.pth'.format(str(n))\n",
    "        torch.save(self.DQN, eval_net_path)\n",
    "        torch.save(self.DQN_target,target_net_path)\n",
    "    def load_model(self,n):\n",
    "        eval_net_path = './{}.pth'.format('model'+str(n))\n",
    "        self.DQN = torch.load(eval_net_path)\n",
    "        self.DQN_target = torch.load(eval_net_path)\n",
    "        print('DQN:load_complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e61fc5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN:load_complete\n",
      "epoch1001 : reward:18.74274324351548\n",
      "epoch1002 : reward:-38.62158752180408\n",
      "epoch1003 : reward:42.559548805749984\n",
      "epoch1004 : reward:-49.65065314519134\n",
      "epoch1005 : reward:24.692362010611113\n",
      "epoch1006 : reward:25.16082126847694\n",
      "epoch1007 : reward:16.44805724356779\n",
      "epoch1008 : reward:32.93027317686041\n",
      "epoch1009 : reward:17.812362851072432\n",
      "epoch1010 : reward:25.967454953814332\n",
      "epoch1011 : reward:14.669130259886927\n",
      "epoch1012 : reward:41.11546005300252\n",
      "epoch1013 : reward:16.185787440204617\n",
      "epoch1014 : reward:-60.48185199262736\n",
      "epoch1015 : reward:42.1385990629135\n",
      "epoch1016 : reward:0.9881573000443815\n",
      "epoch1017 : reward:25.647440366492738\n",
      "epoch1018 : reward:-86.9910033680811\n",
      "epoch1019 : reward:-42.81604252867346\n",
      "epoch1020 : reward:-6.978970362935746\n",
      "epoch1021 : reward:2.421032751784942\n",
      "epoch1022 : reward:40.625286742259675\n",
      "epoch1023 : reward:27.230267040005522\n",
      "epoch1024 : reward:8.550976689399555\n",
      "epoch1025 : reward:47.515757996636594\n",
      "epoch1026 : reward:-27.056468060828188\n",
      "epoch1027 : reward:7.272927743567582\n",
      "epoch1028 : reward:-11.584320444207815\n",
      "epoch1029 : reward:-21.853592347357278\n",
      "epoch1030 : reward:-56.15827559072967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 10000\n",
    "learning_start = 100000\n",
    "batch_size  = 64\n",
    "epsilon = 0.3\n",
    "\n",
    "\n",
    "\n",
    "env = gymnasium.make(\"SoulsGymIudex-v0\")\n",
    "env = env.unwrapped\n",
    "terminated = False\n",
    "action_space = env.action_space\n",
    "\n",
    "N_ACTIONS = env.action_space.n #n個動作可選   s \n",
    "\n",
    "tf_transformer = GameStateTransformer()\n",
    "\n",
    "\n",
    "ep_r_list = []\n",
    "agent = Nstep_DQNAgent(71,action_space = action_space,memory_size = 100000)\n",
    "agent.load_model(1000)\n",
    "losses = 0\n",
    "loss_list = []\n",
    "for i in range(1001,n_epoch):\n",
    "    if i %200 == 0:\n",
    "        agent.save_model(i)\n",
    "    terminated = False\n",
    "    obs, info = env.reset()\n",
    "    obs = tf_transformer.transform(obs)\n",
    "    phase = 1\n",
    "    action_mask = np.zeros(20)\n",
    "    action_mask[info[\"allowed_actions\"]] = 1\n",
    "    rewards = 0\n",
    "    step = 0\n",
    "    while not terminated:\n",
    "        action = agent.act(obs,epsilon,action_mask)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        next_obs = tf_transformer.transform(next_obs)\n",
    "        agent.memory_buffer.push(obs, action, reward, next_obs, terminated)\n",
    "        rewards += reward\n",
    "        step += 1\n",
    "        losses = 0\n",
    "        if agent.memory_buffer.size() >= learning_start:\n",
    "            loss = agent.learn_from_experience(batch_size)\n",
    "            losses += loss\n",
    "            losses.append(loss)\n",
    "        obs = next_obs\n",
    "        action_mask[:] = 0\n",
    "        action_mask[info[\"allowed_actions\"]] = 1\n",
    "    if epsilon >0.2:\n",
    "        pass\n",
    "        #epsilon -= 0.001\n",
    "    if i% 100:\n",
    "        agent.DQN_target.load_state_dict(agent.DQN.state_dict())\n",
    "    try:\n",
    "        print(\"epoch{} : reward:{}\".format(i,rewards))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    ep_r_list.append(reward/step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178b3046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c22735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f9f352",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6834a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291ef779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"array.txt\"\n",
    "\n",
    "# with open(file_path, \"a\") as file:\n",
    "#     for item in all_rewards:\n",
    "#         file.write(str(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f147939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d7e6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70832faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96799777",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86af0561",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db4a626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dc3f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971c0256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
